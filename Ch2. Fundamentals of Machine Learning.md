# 2.1. Rule based machine learning overview

### definition of machine learning

* ê²½í—˜ Eì— ì˜í•´ì„œ ë°°ìš¸ ìˆ˜ ìˆë‹¤ 
* íŠ¹ì • ê³¼ì œ Të¥¼ ìˆ˜í–‰ì„ í•  ìˆ˜ ìˆë‹¤.
* ê²½í—˜ Eì´ ëŠ˜ë©´ ê³¼ì œ Të¥¼ ë” ì˜ ìˆ˜í–‰ Pí•´ë‚¼ ìˆ˜ ìˆë‹¤.

**More experience -> more thumbtack toss, more prior knowledge**

## A perfect world for rule based learning

![image](https://user-images.githubusercontent.com/101063108/157004159-ea13f7da-8e9a-4cae-8162-a34b5b45c420.png)

ê²½í—˜E : ì—¬ëŸ¬ ê´€ì¸¡ë“¤

ê³¼ì œT : Temp, Humid, Wind, Water, Forecast -> EnjoySport

.

ì™„ë²½í•œ ì„¸ìƒì— ëŒ€í•œ ê°€ì •

> ê´€ì¸¡ì˜ ì˜¤ë¥˜ëŠ” ì—†ìŒ. ì¼ê´€ì ì´ì§€ ì•Šì€ ê´€ì¸¡ë„ ì—†ìŒ.

> ëœë¤ì ì¸ ìš”ì†Œê°€ ì—†ìŒ.

> ì‹œìŠ¤í…œì„ ì™„ì „í•˜ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì •ë³´ë“¤ì„ ë‹¤ ìˆ˜ì§‘í•¨.


ì–´ë– í•œ factorê°€ ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê±¸ê¹Œ?


### Fuction Approximation

ë‹¤ì–‘í•œ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œì™€ ê°™ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒ.  

Machnie Learningì€ ë” ë‚˜ì€ ê·¼ì‚¬ í•¨ìˆ˜ë¥¼ ë§Œë“¤ë‚´ë ¤ëŠ” ë…¸ë ¥ì´ë‹¤.

PAC learningê³¼ ë¹„ìŠ· : ë‚®ì€ ì˜¤ë¥˜ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒ.

instance **X** : example -> ì•ì—ì„œì˜ ê²½ìš°ì—” 4ê°œì˜ instanceê°€ ìˆë‹¤ê³  ë§í•  ìˆ˜ ìˆë‹¤.

    * feature O: inputê°’ <sunny, warm, normal, strong, warm, same>
    * label Y: ì‹¤ì œ íŒë‹¨ì˜ ê²°ê³¼ <Yes>

ì´ **X**ë¥¼ ì—¬ëŸ¬ê°œ ëª¨ì•„ë†“ì€ ê²ƒì´ Training Dataset **D**

Hypotheses **H** : ì´ í•¨ìˆ˜ë¼ë©´ì€ í˜„ì‹¤ê³¼ ë™ì¼í•˜ê²Œ ì‘ë™í•  ê²ƒì´ë¼ëŠ” ê°€ì„¤ ì„¤ì •

    h1 : <Sunny, Warm,? ,? ,?, Same> -> Yes
    ì—¬ëŸ¬ê°œì˜ hypothesesê°€ ê°€ëŠ¥í•˜ë‹¤.

Target Function **c**

    ì§„ì§œ í•¨ìˆ˜, ì•Œì§€ ëª»í•˜ì§€ë§Œ ëª©í‘œë¡œ í•˜ì—¬ ì•Œì•„ë‚´ì•¼ í•˜ëŠ” í•¨ìˆ˜
    Hë¥¼ cë¡œ ë§Œë“¤ì–´ì£¼ì–´ì•¼ í•œë‹¤.

![image](https://user-images.githubusercontent.com/101063108/157284223-f0dcc023-8c12-4079-959d-1e3915e931f1.png)

x1 : h1, h2, h3 ëª¨ë‘ í•´ë‹¹

x2 : h1, h2ì—ë§Œ í•´ë‹¹

x3 : h1, h3ì—ë§Œ í•´ë‹¹

    h1ì€ h2, h3ì— ë¹„í•´ generalí•˜ê³ , h2ì™€ h3ëŠ” h1ì— ë¹„í•´ specificí•˜ë‹¤ê³  ë§í•  ìˆ˜ ìˆëŠ”ë°,
    generalí•˜ë‹¤ë©´ instance spaceê°€ ë” í¬ê³ , specificí•˜ë‹¤ë©´ instance spaceê°€ ë” ì‘ë‹¤.


# 2.2. Introduction to Rule Based Algorithm

## Find-S Algorithm

![image](https://user-images.githubusercontent.com/101063108/157349881-8b152230-90ba-499f-8634-10f47dac1f03.png)

> Instance (positiveí•œ ì‚¬ë¡€ë“¤)
* x1 : <Sunny, Warm, Normal, Strong, Warm, Same>
* x2 : <Sunny, Warm, Normal, Light, Warm, Same>
* x3 : <Sunny, Warm, Normal, Strong, Warm, Change>

> Hypotheses
* h0 = <Ã˜, Ã˜, Ã˜, Ã˜, Ã˜, Ã˜>
* h1 : <Sunny, Warm, Normal, Strong, Warm, Same>
* h1,2,3 : <Sunny, Warm, Normal, ?, Warm, Same>
* h1,2,3, 4 : <Sunny, Warm, Normal, ?, Warm, ?>

![image](https://user-images.githubusercontent.com/101063108/157393138-933200b1-81a8-4b28-9239-8a80122c5c3e.png)

ìœ„ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ íŠ¹ì •í•œ hypothesis ì°¾ê³ , function approximationì„ í•´ë‚˜ê°€ëŠ” ê³¼ì •ì´ Find-S Algorithmì´ë‹¤.

## Version Space

ë‹¤ì–‘í•œ ê°€ì„¤ë“¤ì´ ê°€ëŠ¥í•˜ë‚˜ ë”± í•˜ë‚˜ë¡œ ìˆ˜ë ´í•˜ëŠ” ê°€ì„¤ì„ ì°¾ê¸°ëŠ” ì–´ë µë‹¤. ê°€ëŠ¥í•œ ê°€ì„¤ë“¤ì˜ ë²”ìœ„ë¥¼ ì°¾ì•„ì•¼í•œë‹¤. ì´ëŸ¬í•œ ë²”ìœ„ë¥¼ Version Space, **VS** ë¼ê³  í•œë‹¤.

* General Boundary, **G**
    VSì—ì„œ ê°€ì¥ ì¼ë°˜ì ì¸ ê°€ì„¤ì˜ ì§‘í•©
* Specific Boundary, **S**
    VSì—ì„œ ê°€ì¥ êµ¬ì²´ì ì¸ ê°€ì„¤ì˜ ì§‘í•©
* ëª¨ë“  hypothesis, **h**ëŠ” ë‹¤ìŒì„ ë§Œì¡±í•œë‹¤.
    
    ![image](https://user-images.githubusercontent.com/101063108/157395883-8b44f317-a212-4710-a2ef-ff625ebe31f2.png)

![image](https://user-images.githubusercontent.com/101063108/157396036-736f9017-cebb-4b61-9025-6c61cca4596f.png)

> ê°€ì¥ Generalí•˜ê²ŒëŠ” Sunny ë˜ëŠ” Warmë§Œìœ¼ë¡œ ì •í•  ìˆ˜ ìˆì§€ë§Œ, Specificí•˜ê²ŒëŠ” Sunnyì™€ Warm, Strongì´ í¬í•¨ëœ ê°€ì„¤ì„ ì„¸ìš¸ ìˆ˜ ìˆë‹¤.

### Candidate Elimination Algorithm

ê°€ì¥ specificí•œ ê°€ì„¤ê³¼ ê°€ì¥ generalí•œ ê°€ì„¤ì„ ì„¸ìš´ í›„ ì ì  ì¢í˜€ë‚˜ê°€ë©° íŠ¹ì •í•œ VSë¥¼ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜.

ê°€ì¥ specificí•œ ê°€ì„¤ - S : S0: {<Ã˜, Ã˜, Ã˜, Ã˜, Ã˜, Ã˜>}

ê°€ì¥ generalí•œ ê°€ì„¤ - G : G0: {<?, ?, ?, ?, ?, ?>}

![image](https://user-images.githubusercontent.com/101063108/157414110-ffc2bb38-6e74-432c-97a9-b2ef4c7b4cf6.png)

![image](https://user-images.githubusercontent.com/101063108/157433742-02490e48-82ab-4bd9-a3b1-50782973f4c6.png)

![image](https://user-images.githubusercontent.com/101063108/157433636-6cd95436-a096-4282-9328-4c857a75eea4.png)

S1 -> S2 : Humidê°€ Normalê³¼ Highì¼ ë•Œ ë‘˜ ë‹¤ ê²°ê³¼ê°€ positiveí•˜ë¯€ë¡œ ?ë¡œ ë³€ê²½

-> G3 : Rainy or Cold or Changeì¼ë•Œ negativeí•œ ê²°ê³¼. ê³ ë ¤í•´ì„œ hì„¸ìš°ê¸°.

-> S4 : Waterê°€ Warmê³¼ Coolì´ê³  Sameê³¼ Changeì¼ë•Œ ë‘˜ë‹¤ ê²°ê³¼ê°€ positiveí•˜ë¯€ë¡œ ? ë³€ê²½

-> G4 : <?,?,?,?,?,Same>ì€ changeì¼ë•Œ positiveí•˜ë¯€ë¡œ ë§ì§€ ì•ŠìŒ. remove.

ì¤‘ê°„ì—ëŠ” ì—¬ëŸ¬ ê°€ì„¤ë“¤ì´ ìˆê³ , ê·¸ ì¤‘ í•˜ë‚˜ê°€ true function Cì¼ ê²ƒì´ë¼ê³  ê°€ì •í•˜ê³  ë°°ìš°ëŠ” ê²ƒì´ rule based learning ì´ë‹¤. 

ìƒˆë¡œìš´ instanceê°€ ë“¤ì–´ì˜¤ë©´ VSë¥¼ ì¢í˜€ë‚˜ê°ˆ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ ê²ƒì´ë‹¤.

* <Sunny, Warm, Normal, Light, Warm, Same>
ì´ê²ƒì„ ìœ„ì— ì ìš©í•´ë³¸ë‹¤ë©´ Generalì€ ë§ì§€ë§Œ, Specificì€ í‹€ë¦¬ê²Œ ë ê²ƒì´ë‹¤.

ì´ëŸ¬í•œ ê²½ìš°ì— ê²½í—˜, ì‚¬ë¡€ê°€ ì¶©ë¶„í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— íŒë‹¨ì„ ë‚´ë¦¬ì§€ ëª»í•  ìˆ˜ ìˆë‹¤. ë§Œì¡±ì‹œí‚¤ëŠ” ê°€ì„¤ì˜ ê°œìˆ˜ë¥¼ ì„¸ì–´ë³´ëŠ” ë°©ë²•ë„ ìˆì„ ìˆ˜ ìˆì§€ë§Œ í•´ë‹µì´ ë˜ê¸°ëŠ” í˜ë“¤ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— rule based learningì€ ì—¬ëŸ¬ ë¶„ì•¼ì— ë‹¤ì–‘í•˜ê²Œ ì“°ì´ê¸°ê°€ ì‰½ì§€ ì•Šë‹¤.

ë”°ë¼ì„œ 'ì™„ë²½í•œ ì„¸ìƒ'ì—ì„œ ì˜ ì‘ë™í•œë‹¤. candidate-elimination algorithmì€ ë¬´í•œíˆ ë°ì´í„°ê°€ ë“¤ì–´ì˜¬ ë•Œ true functionìœ¼ë¡œ ìˆ˜ë ´ê°€ëŠ¥í•˜ë‹¤.

ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ì™„ë²½í•œ ì„¸ìƒì— ëŒ€í•œ ê°€ì •ì„ ë§Œì¡±í•œë‹¤ë©´,

> ê´€ì¸¡ì˜ ì˜¤ë¥˜ëŠ” ì—†ìŒ. ì¼ê´€ì ì´ì§€ ì•Šì€ ê´€ì¸¡ë„ ì—†ìŒ.

> ëœë¤ì ì¸ ìš”ì†Œê°€ ì—†ìŒ.

> ì‹œìŠ¤í…œì„ ì™„ì „í•˜ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì •ë³´ë“¤ì„ ë‹¤ ìˆ˜ì§‘í•¨.
 
correctí•˜ê²Œ ìˆ˜ë ´í•  ìˆ˜ ìˆë‹¤.

í•˜ì§€ë§Œ ì´ ëª¨ë“  ê²ƒì´ í˜„ì‹¤ì—ì„œ ì´ë£¨ì–´ì§€ê¸°ëŠ” ì–´ë µë‹¤.

ë”°ë¼ì„œ ë°ì´í„°ì˜ ì‚¬ë¡€ì˜ ê°œë³„ featureì—ëŠ” noiseê°€ ë¼ëŠ” ê²ƒì„ ê°ì•ˆí•´ì•¼í•˜ê³ , ë‹¤ë¥¸ ê²°ì •ìš”ì¸ì´ ìˆì„ ìˆ˜ë„ ìˆë‹¤. 

ë…¸ì´ì¦ˆì— ì˜í•´ ë§ëŠ” ê°€ì„¤ì´ ì œê±° ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ í˜„ì‹¤ì˜ ë¬¸ì œì´ë‹¤. ê·¸ë˜ì„œ ë§ë‹¤ ì•„ë‹ˆë‹¤ë¼ê³  ë§í•  ìˆ˜ê°€ ì—†ë‹¤.

# 2.3. Introduction to Decision Tree

ì–´ë–»ê²Œ ì—ëŸ¬ê°€ ìˆëŠ” ë°ì´í„°ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í†µê³„ì ì¸ ê¸°ë²•ì„ ê°€ë¯¸í•´ì„œ learningì„ í•  ìˆ˜ ìˆì„ê¹Œ?

ê°€ì¥ ì ‘ê·¼í•˜ê¸° ì‰¬ìš´ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ê°€ ë°”ë¡œ Decision Tree.

## Decision Tree

ì˜ˆë¥¼ ë“¤ì–´ <sunny,?,?,?,?,?>ë¥¼ decison treeë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.
![image](https://user-images.githubusercontent.com/101063108/157486541-987a78aa-51d8-47be-9f42-252a9b21b4ba.png)

<sunny,warm,?,strong,?,?>ì˜ ê²½ìš°ì—” ë‹¤ìŒê³¼ ê°™ë‹¤.
![image](https://user-images.githubusercontent.com/101063108/157486810-86cad334-d620-4221-bc27-15c204f55249.png)

### Credit Approval Dataset

A1 ~ A15 ê¹Œì§€ 15ê°œì˜ attributeê°€ ìˆê³ , ì´ 690ëª…ì˜ ì‚¬ëŒ ì¤‘ 307ëª…ì—ê²Œë§Œ positiveí•œ ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤ê³  í•˜ì.
![image](https://user-images.githubusercontent.com/101063108/157488231-584c65f8-2638-4c97-b673-1eb6a6703fc2.png)

A1ë§Œì„ ê°€ì§€ê³  íŒë³„í•˜ëŠ” ê²½ìš° 
> aì¼ ê²½ìš° 98ëª…ì´ positiveí•˜ê³  112ëª…ì´ negative 

> bì¼ ê²½ìš° 206ëª…ì´ positiveí•˜ê³  262ëª…ì´ negative

> ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° 3ëª…ì´ positiveí•˜ê³  9ëª…ì´ negative

A9ë§Œì„ ê°€ì§€ê³  íŒë³„í•˜ëŠ” ê²½ìš° 
> tì¼ ê²½ìš° 284ëª…ì´ positiveí•˜ê³  77ëª…ì´ negative 

> fì¼ ê²½ìš° 23ëª…ì´ positiveí•˜ê³  306ëª…ì´ negative

ì–´ë–¤ attributeê°€ feature setì— í¬í•¨ë˜ëŠ” ê²ƒì´ ì¢‹ì„ì§€ ê³ ë¯¼í•´ ë³´ì•„ì•¼í•œë‹¤.


# 2.4. Entropy and Information Gain

## Entropy

ì–´ë–¤ attributeë¥¼ ë” ì˜ ì²´í¬í•  ìˆ˜ ìˆì„ ê²ƒì¸ì§€ ì•Œë ¤ì£¼ëŠ” í•˜ë‚˜ì˜ ì§€í‘œ.

* ë¶ˆí™•ì‹¤ì„±ì„ í™•ì‹¤íˆ ì¤„ì—¬ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤

ì´ëŸ¬í•œ ë¶ˆí™•ì‹¤ì„±ì„ ì¬ëŠ” measure ì¤‘ í•˜ë‚˜ê°€ entropyì´ë‹¤.

ì•ì—ì„œ ë´¤ë˜ A1ì´ë‚˜ A9ê°™ì€ attributeë“¤ì„ íŠ¹ì • í™•ë¥ ë¶„í¬ë¥¼ ê°€ì§€ëŠ” random variableë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ random variableì´ ì–¼ë§ˆë§Œí¼ ë¶ˆí™•ì‹¤ì„±ì´ ë†’ê³  ë‚®ì€ì§€ íŒì •í•˜ëŠ” ê²ƒì´ entropyì´ë‹¤.

ëª¨ë“  ì¼€ì´ìŠ¤ì— ëŒ€í•´ì„œ ë¹„ìŠ·í•œ í™•ë¥  ë¶„í¬ì˜ ê²½ìš°ì—ëŠ” ë¶ˆí™•ì‹¤ì„±ì´ ë†’ë‹¤ ë§í•  ìˆ˜ ìˆê³ , ì–´ë–¤ í•˜ë‚˜ì˜ ì¼€ì´ìŠ¤ê°€ í™•ì‹¤í•˜ê²Œ í•­ìƒ ë‚˜ì˜¨ë‹¤ë©´ ë¶ˆí™•ì‹¤ì„±ì´ ì—†ë‹¤ê³  ë§í•  ìˆ˜ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/101063108/157492110-6c6e7f96-5259-4ed7-91c7-653e16f0ac56.png)

x : case ì´ì‚°ì˜ ê²½ìš°ì—ëŠ” í•©í•˜ë©´ ë˜ì§€ë§Œ, ì—°ì†ì ì´ë¼ë©´ ì ë¶„ì„ í•˜ë©´ ëœë‹¤.

### conditional Entropy

ì–´ë–¤ íŠ¹ì • featureì— ëŒ€í•œ ì •ë³´ê°€ ìˆì„ ê²½ìš°ì— entropyë¥¼ íŒë³„í•˜ëŠ” ê²ƒ.

![image](https://user-images.githubusercontent.com/101063108/157493822-e016724c-3c87-42dc-88a8-190600a06a5f.png)

ì£¼ì–´ì§„ xì— ëŒ€í•´ì„œ yì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•˜ëŠ” ê²ƒ. ë”°ë¼ì„œ ë°‘ì—ì„œì˜ P(X=x)ì˜ summation ë¶€ë¶„ì´ ì¡°ê±´ì— ëŒ€í•´ì„œ ë°˜ì˜í•œ ë¶€ë¶„.

## Information Gain

ì•ì˜ A1ê³¼ A9 attributeì— ëŒ€í•´ entropyë¥¼ ê³„ì‚°í•´ë³´ì.

![image](https://user-images.githubusercontent.com/101063108/157510722-0fcded97-e7ca-4c97-9613-f09090cfb5e6.png)

yê°€ positiveí•œ ê²½ìš° (class variable) í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

P(Y=t) = 307/307+308

H(Y|A1)ì€ A1ì´ë¼ëŠ” ì¡°ê±´ì„ ë¶™ì˜€ì„ ë•Œì˜ conditional entorpyê°’ì´ê³ , H(Y|A9)ëŠ” A9ë¼ëŠ” ì¡°ê±´ì„ ë¶™ì˜€ì„ ë•Œì˜ conditional entropy ê°’ì´ë‹¤.

ì´ë•Œ, Yë¼ê³  í•˜ëŠ” class variableì— ëŒ€í•´ì„œ entropyê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì–´ë–¤ attributeë¥¼ ì¡°ê±´ìœ¼ë¡œ ì¤¬ì„ ê²½ìš° Yì— ëŒ€í•œ entropyê°€ ì–¼ë§Œí¼ ë³€í–ˆëŠ”ì§€ ê·¸ ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ Information Grain(IG)ì´ë‹¤. 

ğ¼ğº(ğ‘Œ,ğ´ğ‘–)=ğ»(ğ‘Œ)âˆ’ğ»(ğ‘Œ|ğ´ğ‘–)

A1ë³´ë‹¤ A9ì´ IGê°€ ë†’ë‹¤.

### ID3 Algorithm

ID3, C4.5, CARTë“± ë‹¤ì–‘í•œ decision treeê°€ ìˆë‹¤. ê·¸ ì¤‘ í•˜ë‚˜ì¸ ID3 Algorithmì— ëŒ€í•´ ì•Œì•„ë³´ì.

![image](https://user-images.githubusercontent.com/101063108/157513974-3531f143-5312-482f-b07d-268afb5a22d4.png)


ìƒˆë¡œìš´ ì˜¤í”ˆ ë…¸ë“œí•˜ë‚˜ë¥¼ ìƒì„±í•˜ê³ , instanceë“¤ì„ ì´ˆê¸° ë…¸ë“œì— ë„£ëŠ”ë‹¤.

ê°€ì¥ ì¢‹ì€ variableì„ ì„ íƒí•´ í•˜ìœ„ì— ì˜¤í”ˆ ë…¸ë“œë¥¼ ìƒì„±í•œë‹¤.
> ì´ë•Œ ì–´ë–¤ attributeë¥¼ ì„ íƒí•˜ë©´ ì¢‹ì„ì§€ëŠ” IGë¥¼ í™œìš©í•˜ë©´ ëœë‹¤. ì´ëŸ° ì‹ìœ¼ë¡œ í•˜ìœ„ì— ì˜¤í”ˆ ë…¸ë“œë“¤ì„ ë§Œë“¤ë©´ ì¢€ ë” í° treeë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

ê° instanceë“¤ì„ ì„ íƒëœ variableì— ë§ê²Œ ë¶„ë¥˜í•˜ê³  ë„£ì–´ì¤€ë‹¤. 

ë§Œì¼ ì •ë ¬ëœ ê²ƒë“¤ì´ ëª¨ë‘ ë™ì¼í•œ classë¼ë©´ ê·¸ ë…¸ë“œë¥¼ ë‹«ëŠ”ë‹¤.

### Problem of Decision Tree

í˜„ì‹¤ì€ 'ì™„ë²½í•œ ì„¸ê³„'ê°€ ì•„ë‹ˆë‹¤. 

![image](https://user-images.githubusercontent.com/101063108/157515591-3e8d3be7-0388-49b4-b8b6-f7978c8d98ec.png)

ì‹¤ì„  : ê¸°ì¡´ì— ê°€ì§€ê³  ìˆë˜ ë°ì´í„°ëŠ” treeì˜ í¬ê¸°ê°€ ì»¤ì§ì— ë”°ë¼ ì •í™•ì„±ë„ ì»¤ì§„ë‹¤.

ì ì„  : ìƒˆë¡œ ì…ë ¥ëœ ë°ì´í„°ëŠ” treeì˜ í¬ê¸°ê°€ ì»¤ì§ì— ë”°ë¼ ì •í™•ì„±ì´ ë–¨ì–´ì§„ë‹¤. ì„¸ì„¸í•œ íŒì •ë“¤ì´ ë§ì•„ì§€ê³  ì—ëŸ¬ì— ëŒ€í•œ ë…¸ì´ì¦ˆê°€ ë§ì•„ì§€ë©´ì„œ, ì •í™•ì„±ì´ ë–¨ì–´ì§„ë‹¤.


# 2.5. How to create a decision tree given a training dataset

ì§€ê¸ˆê¹Œì§€ëŠ” ruleê¸°ë°˜ì˜ ëª¨ë¸ì„ í™œìš©í–ˆë‹¤ë©´ í†µê³„ì ì¸ ë°©ë²•ì— ëŒ€í•´ì„œë„ ì•Œì•„ë³´ì.

Housing datasetì„ ì´ìš©í•´ë³¼í…ë°, 

* 13 numerical independent values (attribute)
* 1 numerical dependent value (class variable)

## Linear Regression

function approximationì„ linearí•œ í˜•íƒœì˜ functionìœ¼ë¡œ approximationí•˜ëŠ” ê²ƒ

hypothesisë¥¼ linearí•œ í•¨ìˆ˜ í˜•íƒœë¡œ ì„¸ìš°ê²Œ ëœë‹¤.

![image](https://user-images.githubusercontent.com/101063108/157518113-0c3630e0-8056-4d27-a785-33d0333d4a66.png)

nì€ feature value ( independent value)ì˜ ê°œìˆ˜

í•¨ìˆ˜ëŠ” linearly weight sum ë¶€ë¶„ê³¼ parameter ğœ½ë¡œ ë‚˜ë‰˜ëŠ”ë°, ìš°ë¦¬ëŠ” ì´ ğœƒë¥¼ ì˜ ì •ì˜í•˜ì—¬ approximationì„ ì§„í–‰í•œë‹¤.

ë” ë‚˜ì€ hypothesisë¥¼ ìœ„í•´ ë” ë‚˜ì€ ğœ½ë¥¼ ì°¾ì•„ì•¼í•œë‹¤.

![image](https://user-images.githubusercontent.com/101063108/157522466-af285720-c9cb-4efc-8e9c-4ee0f3240c4b.png)

x0ë¥¼ 1ì´ë¼ëŠ” dummy variableì„ ë¶™ì—¬ì¤€ë‹¤ë©´, ğœƒ0ì€ summationì— í¬í•¨í•  ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ f hat = Xğœƒ ë¼ëŠ” ê°’ì„ ê°€ì§€ê²Œ ë˜ê³ , ì´ëŠ” í–‰ë ¬ë¡œ í‘œì‹œí•  ìˆ˜ ìˆë‹¤.

   í–‰ë ¬ë¡œ í‘œì‹œí•˜ë©´ XëŠ” dummy variable 1ì„ í¬í•¨í•˜ì—¬ ì´ Dê°œì˜ ë°ì´í„°ì…‹ê³¼ nê°œì˜ attributesë¥¼ ê°€ì§„ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. 

ê·¸ëŸ¬ë‚˜ í˜„ì‹¤ì—ì„œëŠ” ì—ëŸ¬ê°€ ê»´ì ¸ìˆë‹¤. f hatê³¼ fì˜ ê°€ì¥ í° ì°¨ì´ëŠ” ì´ ì—ëŸ¬ì˜ ìœ ë¬´ì´ë‹¤. f hatì— ê²½ìš°, ì—ëŸ¬ê°€ ì‹ì— í¬í•¨ë˜ì§€ ì•ŠëŠ”ë‹¤. fê°€ í˜„ì‹¤ì— ë§ëŠ” true functionì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

![image](https://user-images.githubusercontent.com/101063108/157523354-2f1553c5-112c-444f-9e4c-ef500cfa8a9d.png)

ì•ìª½ì˜ summation ë¶€ë¶„ì´ f hatê³¼ ì¼ì¹˜í•˜ë¯€ë¡œ f = Xğœƒ + e = Yë¼ê³  í•  ìˆ˜ ìˆë‹¤.

ğœƒëŠ” ì—ëŸ¬ê°€ ê°€ì¥ ì‘ì„ë•Œë¡œ í•´ì•¼í•œë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì´ ì „ê°œëœë‹¤.

![image](https://user-images.githubusercontent.com/101063108/157523755-13a1d32b-4587-427d-ad77-80a68072eac2.png)

ğœƒ hatë˜í•œ ë§ˆì°¬ê°€ì§€ë¡œ í˜„ì‹¤ì˜ ì—ëŸ¬ë¥¼ ì œì™¸í•œ ë¶€ë¶„ì´ë‹¤. í–‰ë ¬ì˜ ì›ë¦¬ë¥¼ í™œìš©í•´ ì‹ì„ ë¶„í•´í•˜ê³ , ğœƒê°€ ë“¤ì–´ìˆì§€ ì•Šì€ ë¶€ë¶„ì€ í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ ì‹ì—ì„œ ì œì™¸í•œë‹¤.

ì´ì œ ğœƒë¥¼ ìµœì í™”í•´ë³´ì.

### Optimized ğœƒ

ìœ„ì˜ ğœƒ hatì˜ ì‹ì„ ê·¹ê°’, ì¦‰ ë¯¸ë¶„ì„ í™œìš©í•œë‹¤.

![image](https://user-images.githubusercontent.com/101063108/157532292-388a5530-cbc9-4fb9-b860-b829e40c8423.png)

ìš°ë¦¬ëŠ” X, Yê°’ì„ ì•Œê³ ìˆìœ¼ë¯€ë¡œ ğœƒë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.

housing dataì— ëŒ€í•´ì„œ linear regressionì„ í•´ë³´ì. (ì²«ë²ˆì§¸ feature)

![image](https://user-images.githubusercontent.com/101063108/157533388-449b3cf1-bde2-43a8-abd0-e1c8eee6bc35.png)


Red : dependent variableì˜ ë¶„í¬ (f)

xë¼ê³  í•˜ëŠ” í•˜ë‚˜ì˜ ê°’ì— ëŒ€í•´ì„œ ğœƒëŠ” ë‘ê°œê°€ ë‚˜ì˜¨ë‹¤. (dummy variableë¡œ ë“¤ì–´ê°„ê²ƒ í•˜ë‚˜: ğœƒ0, featureì— ëŒ€í•´ì„œ í•˜ë‚˜: ğœƒ1)

blue: ğœƒ0ì— ì˜í•´ ì ˆí¸, ğœƒ1ì— ì˜í•´ ê¸°ìš¸ê¸°ê°€ ìƒì„±ë˜ì–´ linearí•˜ê²Œ ë§Œë“¤ì–´ì§. (f hat)

ë’¤ì— ìˆëŠ” ë¶€ë¶„ê¹Œì§€ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œëŠ” linearí•œ ê°€ì •ì„ ìˆ˜ì •í•´ì•¼í•œë‹¤.

![image](https://user-images.githubusercontent.com/101063108/157534538-c30c6cf0-8d7e-4a04-a80d-312dc69f3efc.png)

![image](https://user-images.githubusercontent.com/101063108/157534939-fbfe4fc3-5175-473c-a63f-6a8b506712ba.png)

ìœ„ì˜ linear regressionì´ë‘ ê°™ì§€ë§Œ xê°’ì„ xì˜ ì œê³±, ì„¸ì œê³±,... í•´ì„œ ìˆ˜ë¥¼ ëŠ˜ë ¤ì£¼ëŠ” ë¶€ë¶„ì´ ì¶”ê°€ ëœ ê²ƒ. (ì„ í˜•ì´ ì•„ë‹˜)

![image](https://user-images.githubusercontent.com/101063108/157535330-a8797ccc-dbb5-4c6e-a244-0f98af110396.png)

    redì˜ trendì— ë” ì˜ ë§ìŒ. 
    ê·¸ëŸ¬ë‚˜ ë’¤ì˜ ë¶€ë¶„ì´ ê´€ì¸¡ì¹˜ì˜ ìˆ˜ê°€ ì ì€ë° ì´ë¥¼ ìœ„í•´ ìŠ¹ìˆ˜ë¥¼ ë†’ì¸ë‹¤ëŠ” ê²ƒì€ ë…¸ë“œë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒ 
    (íŠ¸ë¦¬ ì‚¬ì´ì¦ˆë¥¼ í‚¤ìš°ëŠ” ê²ƒ)ê³¼ ê°™ë‹¤. ê¸°ì¡´ì— ìˆëŠ” ê²ƒì€ ì˜ ì„¤ëª… í•  ìˆ˜ ìˆì§€ë§Œ, ë¯¸ë˜ì— ì˜¬ ê°’ì— 
    ëŒ€í•´ ì˜ ì„¤ëª… í• ì§€ëŠ” ì˜ë¬¸.


### Limitations

* data setì´ ë§ì•„ì§€ë©´ ë§ì•„ì§ˆìˆ˜ë¡ ëª¨ë¸ì´ ë³µì¡í•´ì§€ê³  ì—ëŸ¬ê°€ ëŠ˜ì–´ë‚  ê²ƒ
* í•œê³„ê°€ ìˆëŠ” ëª¨ë¸ë“¤ì´ì§€ë§Œ ê¸°ì´ˆê°€ ë˜ëŠ” ëª¨ë¸.




